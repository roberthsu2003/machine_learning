# 支援向量機 SVC（Support Vector Classifier）

## 重點:
- 解釋支援向量機的基本概念，包括最大邊界、支援向量和核函數。
- 解釋核函數的意義，以及如何使用不同的核函數來處理非線性分類問題。
- 解釋懲罰參數 C 的意義，以及它如何影響模型的複雜度和泛化能力。
- 修改核函數和 C 值，觀察模型的變化。
- 介紹其他評估指標，例如精確度、召回率和 F1 分數。
- 嘗試使用其他數據集，例如手寫數字數據集，來進行 SVM 分類分析。
- 解釋 SVM 的優缺點，例如在高維空間中表現良好、但對參數敏感。
- 討論 SVM 的應用場景，例如圖像分類和文本分類。
- 解釋什麼是支持向量，以及支持向量在模型中的重要性。

在線性模型分類中有討論的線性模型支援向量分類,而核化支援向量機（通常簡稱為 SVM）是一種模型擴展，它允許更複雜的模型，而這些模型不是簡單地由輸入空間中的超平面定義的。雖然有用於分類和回歸的支援向量機，但我們將限制於分類情況，如在 `SVC` 中實現分類。類似的概念適用於支援向量迴歸，如在 `SVR` 中實現迴歸。

核化支援向量機背後的數學有點複雜.

## 線性模型和非線性特徵

線性模型在低維空間中受到很大限制，因為線和超平面的靈活性有限。使線性模型更靈活的一種方法是添加更多特徵 - 例如，添加輸入特徵的交互作用或多項式。

### ➜線性可分的定義：
- 如果資料可以用一條直線（在2D空間）或一個超平面（在高維空間）完全分開，那麼這些資料就是線性可分的。
- 在這種情況下，使用線性模型（如線性SVM）是合適的。

### ➜如何判斷是否線性可分：
- 在2D空間中，我們可以通過視覺化來判斷
- 如果不同類別的點可以用一條直線分開，就是線性可分
- 如果不同類別的點交錯在一起，無法用直線分開，就是非線性可分

### ➜為什麼要使用線性模型：
- 為什麼要使用線性模型：
- 線性模型計算效率高
- 解釋性好
- 不容易過擬合
- 如果資料確實是線性可分的，線性模型就能得到很好的結果

### ➜如果資料不是線性可分的：
- 我們可以使用核化技巧（Kernel Trick）
- 將資料映射到更高維的空間，使其變得線性可分
- 常用的核函數包括：RBF核、多項式核等

> [!IMPORTANT]
> 以下範例解釋低維度下不可以線性分割的範例
> [svm_1無法使用線性分割的範例](./svm_1無法使用線性分割的範例.ipynb)
> ---
> 以下範例是增加1個維度,變為3維後,使用高維分類的方式
> [svm_2增加1個特徵成為可以高維度分割範例](./svm_2增加1個特徵成為可以高維度分割範例.ipynb)


## 進階:
- 不同核函數的原理，例如多項式核函數和徑向基核函數（RBF）。
- 可以講解如何使用交叉驗證來選擇最佳的參數組合。
- 如何使用 SVM 進行迴歸分析（SVR）。

### 何時使用 Support Vector Classifier(Linear SVC)?

**➜適合情況**:   
1. 資料在原始特徵空間中大致線性可分。
2. 資料維度(特徵數)遠大於樣本數(例如文字分類,TF-IDF)
3. 想要模型可解釋性較高
4. 速度考量: 線性 SVM 通常比 kernel SVM 快得多，尤其是資料量大時。
5. 想做特徵選擇(可搭配L1 Regularization)

```
from sklearn.svm import SVC
clf = SVC(kernel='linear')
```

### 何時使用 Support Vector Classifier(Linear SVC)?

**➜適合情況**:

1. 資料在原始空間中不是線性可分,需要更靈活的decision boundary
2. 資料數量不是很大(kernel方法計算量大)
3. 想利用RBF, Polynomial 或 Sigmoid Kernel 來捕捉非線性關係
4. 資料有複雜的形狀或結構(例如螺旋,環狀)

```python
from sklearn.svm import SVC
clf = SVC(keranl='rbf')
```

## 實作:
[**SVC(Support Vector Classifier）model實作**](./sklearn實作1.ipynb)