**Bagging 的核心概念**

Bagging 的中文意思是「套袋法」，它是一種並行式的集成學習方法。它的主要思想是通過減少模型的方差 (variance) 來提高預測的準確性和穩定性。想像一下，如果你只問一個專家意見，他可能會因為某些個人偏見或知識盲點而給出有偏差的答案。但如果你問一群專家，然後綜合他們的意見，結果通常會更可靠。Bagging 就是類似的道理。

它主要包含兩個關鍵步驟：

1. **Bootstrap (自助法/引導聚集抽樣)**:
    - 這是指從原始訓練數據集中**有放回地隨機抽樣**多次，創建出多個與原始數據集大小相同的「自助樣本集」(bootstrap samples)。
    - 「有放回抽樣」意味著同一個數據點在同一個自助樣本集中可能出現多次，也可能一次都不出現。
    - 由於是隨機抽樣，每個自助樣本集都會略有不同，這就為訓練出多樣性的基礎模型提供了基礎。
    - 通常，一個自助樣本集大約會包含原始數據中 63.2% 的獨特樣本 (當樣本數趨近於無窮大時)。剩下的約 36.8% 的樣本則沒有被抽到，這些樣本被稱為「袋外樣本」(Out-Of-Bag, OOB samples)，可以用來做模型的內部驗證，類似於交叉驗證。
1. **Aggregating (聚合)**:
    - 對於每個自助樣本集，我們都會獨立地訓練一個基礎模型（例如決策樹）。由於每個模型是在略有不同的數據上訓練的，它們的預測結果也會有所不同。
    - 在所有基礎模型都訓練完成後，Bagging 會將它們的預測結果聚合起來，形成最終的預測。
        - **對於分類問題**：通常使用「投票法」(voting)。例如，如果多數模型預測某個樣本屬於類別 A，那麼最終結果就是類別 A。
        - **對於回歸問題**：通常使用「平均法」(averaging)。例如，將所有模型對某個樣本的預測值取平均，作為最終的預測值。

**隨機森林 (Random Forest) 作為 Bagging 的特例**

隨機森林是 Bagging 的一個非常成功和流行的擴展。它在 Bagging 的基礎上，進一步引入了隨機性：

- 除了對樣本進行自助抽樣外，在訓練每棵決策樹的每個節點時，隨機森林**不是考慮所有特徵**來進行分裂，而是**隨機選擇一部分特徵** (feature subset) 進行考慮。
- 這樣做的目的是進一步增加模型之間的多樣性，減少模型之間的相關性，從而進一步降低過擬合的風險。

**一個簡單的假設性小範例 (分類問題)**

假設我們有一個非常小的數據集，用來預測一個水果是「蘋果」還是「橘子」。我們的特徵只有兩個：「顏色」（紅/橙）和「形狀」（圓/橢圓）。

**原始數據集 (D):**

| **水果編號** | **顏色** | **形狀** | **類別** |
| -------- | ------ | ------ | ------ |
| 1        | 紅      | 圓      | 蘋果     |
| 2        | 橙      | 圓      | 橘子     |
| 3        | 紅      | 橢圓     | 蘋果     |
| 4        | 橙      | 圓      | 橘子     |
| 5        | 紅      | 圓      | 蘋果     |

現在我們使用 Bagging，假設我們要訓練 3 個基礎模型 (例如，非常簡單的決策樹，我們稱它們為 M1, M2, M3)。

**步驟 1: Bootstrap 抽樣**

我們從原始數據集 D 中進行有放回抽樣，生成 3 個自助樣本集 (D1, D2, D3)，每個樣本集大小與 D 相同 (5個樣本)：

- **D1 (自助樣本集1):** 可能抽到 {水果1, 水果3, 水果1, 水果5, 水果2}
    - 訓練出的模型 M1 可能學到：「紅色且圓形」很可能是蘋果。
- **D2 (自助樣本集2):** 可能抽到 {水果4, 水果2, 水果2, 水果5, 水果1}
    - 訓練出的模型 M2 可能學到：「橙色且圓形」很可能是橘子。
- **D3 (自助樣本集3):** 可能抽到 {水果3, 水果5, 水果4, 水果1, 水果3}
    - 訓練出的模型 M3 可能學到：「紅色」傾向於是蘋果，「橢圓」也傾向於是蘋果。

注意：由於是有放回抽樣，D1 中水果1出現了兩次，水果4沒有出現。D2 中水果2出現了兩次，水果3沒有出現。

**步驟 2: 訓練基礎模型**

我們用 D1 訓練 M1，用 D2 訓練 M2，用 D3 訓練 M3。由於訓練數據不同，這三個模型會學到不同的規則。

**步驟 3: Aggregating (聚合預測)**

現在來了一個新的水果需要預測：**新水果 X：顏色=紅，形狀=圓**

我們讓三個模型分別預測：

- **M1 (基於 D1) 預測:** 蘋果 (因為 D1 中有較多紅色且圓形是蘋果的例子)
- **M2 (基於 D2) 預測:** 蘋果 (可能因為 D2 中也有紅色圓形的蘋果，或者它的規則不夠強烈反對)
- **M3 (基於 D3) 預測:** 蘋果 (因為 D3 中紅色是蘋果的特徵比較明顯)

**最終預測 (投票):**

由於三個模型都預測「蘋果」，通過多數投票，Bagging 的最終預測結果就是「蘋果」。

**如果有一個模型預測不同呢？**

假設 M2 預測的是「橘子」：

- M1: 蘋果
- M2: 橘子
- M3: 蘋果

投票結果：蘋果 (2票) vs 橘子 (1票)。最終預測仍然是「蘋果」。

**這個小範例說明了：**

1. **數據擾動**：通過 Bootstrap 抽樣，每個基礎模型看到的數據略有不同。
2. **模型多樣性**：不同的數據導致訓練出略有不同的模型。
3. **集體決策**：通過聚合（如投票），可以綜合多個模型的「意見」，減少單個模型可能因數據片面性導致的錯誤，從而得到更穩健的結果。

如果我們只用原始數據集 D 訓練一個模型，該模型可能會對數據中的某些特定模式過度敏感。而 Bagging 通過引入隨機性和聚合，使得整體模型更加魯棒，不容易受到訓練數據中微小變化的影響。

### 魯棒(Robust)的意思是:
通過 Bagging 這種方法，我們最終得到的集成模型（整體模型）對於數據中的微小變動、噪聲或異常值不那麼敏感，並且在新的、未見過的數據上通常能表現得更穩定和可靠。