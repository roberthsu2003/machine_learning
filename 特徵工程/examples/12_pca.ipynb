{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 30) (3197710343.py, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 30\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(\"\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 30)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 降維 (Dimensionality Reduction) ---\n",
    "# 降維是指在盡可能保持數據結構的情況下，減少特徵的數量。\n",
    "# 降維的好處：\n",
    "# - 減少計算成本和儲存空間。\n",
    "# - 降低模型過擬合的風險。\n",
    "# - 方便數據視覺化 (例如降到 2D 或 3D)。\n",
    "\n",
    "# --- 主成分分析 (Principal Component Analysis, PCA) ---\n",
    "# PCA 是一種最常用的線性降維方法。\n",
    "# 它的目標是找到一組新的互相垂直的特徵 (稱為主成分)，\n",
    "# 這些主成分是原始特徵的線性組合。\n",
    "# PCA 會選擇那些能夠最大化數據變異數的主成分。\n",
    "\n",
    "# 建立一個分類問題的範例數據集\n",
    "X, y = make_classification(n_samples=200, n_features=20, n_informative=5,\n",
    "                           n_redundant=5, n_classes=3, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "# 將數據轉換為 DataFrame (方便觀察)\n",
    "X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(20)])\n",
    "\n",
    "print(\"原始數據的維度:\")\n",
    "print(X.shape)\n",
    "print(\"\n",
    "\")\n",
    "\n",
    "# --- 步驟一：數據標準化 ---\n",
    "# PCA 對特徵的尺度非常敏感，所以在進行 PCA 之前，\n",
    "# 通常需要先對數據進行標準化。\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# --- 步驟二：應用 PCA ---\n",
    "# 建立 PCA 物件\n",
    "# n_components: 要保留的主成分數量\n",
    "# 如果 n_components 是一個 0 到 1 之間的浮點數，\n",
    "#   它表示希望保留的變異數比例。\n",
    "#   例如，n_components=0.95 表示保留 95% 的變異數。\n",
    "# 如果 n_components 是一個整數，它表示要保留的主成分數量。\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# 對標準化後的數據進行擬合與轉換\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(\"PCA 降維後的數據維度:\")\n",
    "print(X_pca.shape)\n",
    "print(\"\n",
    "\")\n",
    "\n",
    "# --- 分析 PCA 結果 ---\n",
    "# explained_variance_ratio_ 屬性顯示了每個主成分所解釋的變異數比例。\n",
    "print(\"每個主成分解釋的變異數比例:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(f\"總共解釋的變異數比例: {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
    "print(\"\n",
    "\")\n",
    "\n",
    "# components_ 屬性顯示了主成分與原始特徵之間的關係。\n",
    "# (行是主成分，列是原始特徵)\n",
    "print(\"主成分與原始特徵的關係 (權重):\")\n",
    "print(pca.components_)\n",
    "print(\"\n",
    "\")\n",
    "\n",
    "# --- 視覺化 PCA 結果 ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', alpha=0.8)\n",
    "plt.title('PCA of Classification Data (2 Components)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=list(np.unique(y)))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
