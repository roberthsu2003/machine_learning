# 機器學習大綱

## 一、 基礎概念
### 名詞解釋
#### 監督式學習（Supervised Learning）和 非監督式學習（Unsupervised Learning）

在機器學習中，監督式學習（Supervised Learning）和非監督式學習（Unsupervised Learning）是兩種主要的學習範式。

---

##### 1. 監督式學習（Supervised Learning）
**描述**：
監督式學習是指使用帶有標籤（即已知輸出或目標變量）的訓練數據來訓練模型。模型通過學習輸入特徵與對應標籤之間的關係，來預測新數據的輸出。監督式學習的目標是讓模型能夠對未見數據進行準確預測或分類。

**特徵**：
- 數據包含輸入特徵 \( X \) 和對應的標籤 \( y \)。
- 訓練目標是最小化預測值與真實標籤之間的誤差。
- 主要任務包括：
  - **分類（Classification）**：預測離散類別（如垃圾郵件或非垃圾郵件）。
  - **回歸（Regression）**：預測連續值（如房價）。

**簡單範例**：
假設你有一個數據集，包含房屋的面積、房間數量和價格（標籤）。使用線性回歸模型進行監督式學習，模型學習面積和房間數量與價格之間的關係。訓練後，模型可以根據新房屋的面積和房間數量預測其價格。例如：
- 輸入：面積 = 100 平方米，房間數 = 2
- 標籤：價格 = 500 萬
- 模型預測新房屋的價格，實現回歸任務。

**常見算法**：
- 線性回歸（Linear Regression）
- 邏輯回歸（Logistic Regression）
- 支援向量機（Support Vector Machine, SVM）
- 決策樹（Decision Tree）
- 神經網絡（Neural Networks）

---

##### 2. 非監督式學習（Unsupervised Learning）
**描述**：
非監督式學習是指在沒有標籤的數據上進行訓練，模型通過發現數據中的內在結構、模式或關係來學習。非監督式學習的目標是探索數據的隱含分佈或分組，而非預測特定輸出。

**特徵**：
- 數據僅包含輸入特徵 \( X \)，無對應標籤 \( y \)。
- 主要任務包括：
  - **聚類（Clustering）**：將數據分為相似的群組（如客戶分群）。
  - **降維（Dimensionality Reduction）**：將高維數據簡化為低維表示（如主成分分析）。
- 模型學習數據的分佈或結構，而非直接預測。

**簡單範例**：
假設你有一個包含客戶購買行為的數據集（例如購買的產品類型和頻率），但沒有標籤。使用 K 均值聚類（K-means Clustering）算法進行非監督式學習，模型將客戶分為若干群組（如高消費群、低消費群）。例如：
- 輸入：客戶 A 的購買記錄（10 次購買電子產品，5 次購買服裝）
- 模型輸出：客戶 A 屬於“科技愛好者”群組。
- 這種分組有助於市場營銷策略的制定。

**常見算法**：
- K 均值聚類（K-means Clustering）
- 層次聚類（Hierarchical Clustering）
- 主成分分析（PCA, Principal Component Analysis）
- 自編碼器（Autoencoders）
- 關聯規則挖掘（Association Rule Mining）

---
### 機率統計
#### 隨機變數（Random Variable)
- 說明1:隨機變數是對隨機事件結果的數值描述。例如，擲骰子的結果可以用一個數值來表示（如1到6）。
- 說明2:是一個將隨機實驗結果對應到數值的函數

#### 離散隨機變數(Discrete Random Variable)
是指「所有可能的取值只有有限個或可數無窮個」的隨機變數，例如擲骰子的點數（1, 2, 3, 4, 5, 6）。

#### 連續隨機變數
連續隨機變數（Continuous Random Variable）是指「取值為某一區間內的實數，且這些數值是不可數無限多個」的隨機變數。也就是說，連續隨機變數的可能值無法一一列舉，而是「遍布」在某個或整個實數區間內

#### 常見機率分佈
- **常態分佈（Normal Distribution）** - 連續分佈
- **均勻分佈（Uniform Distribution）** - 連續分佈
- **二項分佈（Binomial Distribution）** - 離散分佈

---
### 機器學習常使用的圖表
- **散佈圖(scatter_plot)**: 觀察資料的分佈
- **混淆矩陣(Confusion Matrix)**: 它是一種在機器學習中用來評估分類模型效能的表格。
- **決策邊界(Decision Boundary)**: 線性分類模型在建立決策邊entery中常使用

---
### 基本 Package 與工具
**需要安裝的套件**
`$ pip install jupyter numpy scipy matplotlib ipython scikit-learn pandas mglearn`

- **numpy**: 科學計算庫
- **scipy**: 科學計算庫
- **matplotlib**: 繪圖庫
- **ipython**: 互動式計算
- **scikit-learn**: 機器學習庫
- **pandas**: 數據分析庫
- **mglearn**: 機器學習輔助庫

## 二、 數據處理與特徵工程
### 使用數據集
#### forge數據集
- two-class classification(2個種類的分類)
#### wave數據集
- regression 迴歸的演算法
#### Scikit-learn的Iris 數據
- Scikit-learn 內建的 Iris (鳶尾花) 數據集
#### Wisconsin Breast Cancer dataset(威斯consin州乳癌資料集)
#### 加州房價數據集 (Scikit-learn) 與 Ames 房價數據集 (OpenML)

---
### 特徵工程
特徵工程是利用領域知識來建立特徵，讓機器學習演算法得以運作的過程。它包含了特徵的創造、轉換、提取和選擇。
- **數據預處理 (處理缺失值、數據清洗)**
- **處理類別數據 (標籤編碼、獨熱編碼)**
- **特徵縮放 (標準化、歸一化)**
- **特徵創造 (多項式特徵、互動特徵)**
- **特徵選擇 (過濾法、包裝法、嵌入法)**
- **降維 (主成分分析 PCA)**

## 三、 監督式學習 - 迴歸模型
### 簡單線性迴歸
線性模型是一類在實踐中廣泛使用的模型，在過去的幾十年中得到了廣泛的研究，其根源可以追溯到一百多年前年。
對於迴歸，線性模型的一般預測公式如下：
`ŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b`
這裡，x[0] 到 x[p] 表示單一資料點的特徵（本例中特徵個數為 p），w 和 b 是模型學習到的參數，ŷ 是模型所做的預測。

### 多元線性迴歸
多元線性迴歸是簡單線性迴歸的擴展，用於預測一個應變數與多個自變數之間的關係。
- **Label Encoding**: 將文字轉為數值，用於有序類別特徵。
- **One Hot Encoding**: 將文字轉為數值，用於無序類別特徵。
- **特徵縮放(Feature Scaling)**: 加速gradient descent。

## 四、 監督式學習 - 分類模型
### k-近鄰分類 (k-NN)
k 近鄰分類 (k-Nearest Neighbors, KNN) 是一種簡單直觀的分類方法。
- **k 值**: k 值代表我們要找多少個鄰居。k 值的選擇會影響分類結果。
- **距離計算**: 計算距離的方法有很多種，例如歐氏距離。
- **優點**: 容易理解和實作。對於多種類別的分類效果不錯。
- **缺點**: 計算量可能較大。容易受到資料雜訊的影響。

### 邏輯迴歸
邏輯迴歸主要用於解決分類問題，也就是預測一個事件屬於哪個類別。
- 與線性迴歸不同，邏輯迴歸的輸出不是連續的數值，而是介於 0 和 1 之間的機率值，表示某事件發生的可能性。
- 邏輯迴歸使用 Sigmoid 函數（或稱為 Logistic 函數）將線性迴歸的輸出轉換為機率值。

### 貝氏分類
貝氏分類（Naive Bayes Classifier）是機器學習中非常經典且直觀的分類演算法。
- **GaussianNB(高斯單純貝氏)**: 適用連續數據。
- **BernoulliNB(伯努利單純貝氏)**: 適用二元數據。
- **MultinomialNB(多元單純貝氏)**: 適用計數數據。

### 核化支援向量機 (SVM)
核化支援向量機（通常簡稱為 SVM）是一種模型擴展，它允許更複雜的模型，而這些模型不是簡單地由輸入空間中的超平面定義的。
- **核技巧**: 可以在不直接計算新的高維特徵表示的情況下，學習高維空間中的分類器。
- **gamma**: 控制高斯核的寬度。
- **C**:正規化參數。

### 樹狀模型 (決策樹)
判斷樹是廣泛用於分類和迴歸任務的模型。本質上，它們學習 if/else 問題的層次結構，從而得出決策。
- **預修剪(pre-pruning)**: 提前停止樹的創建。
- **後修剪(post-pruning)**: 建立樹，然後刪除或折疊包含很少資訊的節點。

## 五、 進階模型與技巧
### 薈萃式學習 (Ensemble Learning)
薈萃式學習（集成學習）是一種機器學習方法，通過結合多個基礎模型（稱為弱學習器或基學習器）的預測結果來提高整體模型的性能。
- **Bagging (Bootstrap Aggregating)**: 如隨機森林（Random Forest），通過對數據進行有放回採樣訓練多個模型，並平均或投票得出結果。
- **Boosting**: 如AdaBoost、Gradient Boosting和XGBoost，通過迭代地調整樣本權重或梯度來訓練模型，逐步提升性能。
- **Stacking**: 將多個不同類型的模型預測結果作為輸入，訓練一個元模型（meta-model）進行最終預測。

### 決策樹集成模型 (Random Forest)
隨機森林是決策樹的集合，其中每棵樹都與其他樹略有不同。
- **自舉抽樣(bootstrapping)**: 導致隨機森林中的每個決策樹都建立在略有不同的資料集上。
- **特徵選擇**: 每個節點中的特徵選擇，每棵樹中的每個分割都會對不同的特徵子集進行操作。

### 深度學習
深度學習是機器學習的一個分支，它使用多層神經網絡來學習數據的複雜模式。
- **Tensorflow**: Google 開發的開源機器學習框架。
- **Pytorch**: Facebook 開發的開源機器學習框架。

## 六、 非監督式學習
### 集群分析 (Clustering / k-means)
集群分析（Clustering），又稱聚類分析，是一種非監督式學習方法。
- **K-means**: 將資料分成預先定義的 K 個群集。
- **階層式集群（Hierarchical Clustering）**: 建立一個群集的階層結構。
- **DBSCAN**: 基於資料點的密度進行分群。

## 七、 模型評估指標
### 準確率 (Accuracy)
正確分類的樣本數佔總樣本數的比例。
`Accuracy = (TP + TN) / (TP + TN + FP + FN)`

### 精確率 (Precision)
在所有預測為正類的樣本中，實際為正類的比例。
`Precision = TP / (TP + FP)`

### 召回率 (Recall)
在所有實際為正類的樣本中，被正確預測為正類的比例。
`Recall = TP / (TP + FN)`

### F1-Score
精確率和召回率的調和平均數。
`F1-score = 2 * (Precision * Recall) / (Precision + Recall)`

### R-squared
衡量模型解釋數據變異性的比例。

### MAE, MSE, RMSE
- **MAE (Mean Absolute Error)**: 平均絕對誤差
- **MSE (Mean Squared Error)**: 均方誤差
- **RMSE (Root Mean Squared Error)**: 均方根誤差

### 混淆矩陣 (Confusion Matrix)
以表格形式展示 TP、TN、FP、FN 的數量，全面呈現模型的預測性能。

### AUC - ROC 曲線
接收者操作特徵曲線 (ROC) 下的面積，用於評估二元分類器的整體性能。